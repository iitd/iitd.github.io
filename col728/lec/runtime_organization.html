<h3>Runtime Organization</h3>

<p>Introduction
<ul>
	<li>We have covered the front-end phases: lexing, parsing, semantic analysis
	<li>Now back-end: optimization, code generation
	<li>Before that: we need to talk about runtime organization --- what do we need to generate. Later we will talk about how to generate it (code generation)
</ul>


<p>What lives where?

<p>What:
<ul>
  <li> code</li>
  <li> variables: local and global</li>
  <li> malloc'd memory</li>
</ul>

<p>Where?
<ul>
<li> Memory address space (how to manage, what is the layout)
<li> registers
<li> disk
</ul>

<p>Who manages what?

<p>Who
<ul>
  <li> code generated by compiler</li>
  <li> OS</li>
  <li> runtime libraries</li>
  <li> hardware</li>
</ul>

<p>Execution of a program is initially under the control of the operating system
<ul>
	<li>The OS allocates space for a program
	<li>Code is loaded into part of the space
	<li>OS jumps to the entry point (e.g., "main")
  <li>Other space is used for <em>data</em>. Some of the data space is initialized by the OS, rest is managed by compiler-generated code and/or libraries.
  <li>The data space can be <em>dynamically (de)allocated</em> using calls to the OS.
</ul>
Compiler is responsible for (a) generating code, and (b) orchestrating the use of the data area.

<p>Draw a picture of memory with "code" and "other space".
<ul>
	<li>Will draw memory as a rectangle with low address at bottom and high address at top
	<li>Lines delimiting different areas of the memory
	<li>Simplification: assume contiguous memory (need not be necessary)
</ul>

<h3>Programming model</h3>
We will assume the following programming model:
<ul>
<li>Sequential execution</li>
<li>Procedure call/return semantics</li>
</ul>
For now, we are ignoring the following programming constructs:
<ul>
<li> Exceptions (throw/catch)</li>
<li> call/cc : call with current continuation</li>
</ul>
These additional programming
constructs will depend on the ideas we will discuss.

<h3>Activations</h3>

<ul>
	<li>An invocation of procedure <code>P</code> is an <em>activation</em> of <code>P</code>
	<li>The <em>lifetime</em> of an activation of <code>P</code> is
	<ul>
		<li>All the steps to execution <code>P</code>
		<li>Including all the steps in procedures <code>P</code> calls
	</ul>
	<li>The <em>lifetime</em> of a variable <code>x</code> is the portion of execution in which <code>x</code> is defined, which includes:
	<li>Note that
	<ul>
		<li>Lifetime is a dynamic (run-time) concept
		<li>Scope is a static concept
    <li>Many simultaneous runtime instances of the same procedure or variable are possible.
    <li>Lifetimes often cross scope boundaries
	</ul>
	<li>Observation
	<ul>
		<li>When P calls Q, then Q returns before P returns
	</ul>
	<li>Lifetimes of procedure activations are properly nested. Draw figure of P's lifetime and how Q's lifetime is a sub-interval of that.
	<li>Activation lifetimes can be depicted as a tree
	<li>Consider an example:
	<pre>
int g() { return 1; }
int f() { return g(); }

int
main() {
g();
f();
}
	</pre>
  Draw the tree of activation lifetimes: <code>main-&gt;g</code>, <code>main-&gt;f</code>, <code>f-&gt;g</code>.
	<ul>
		<li>The example shows nesting and the fact that different calls to the same procedure will have different activations
	</ul>
	<li>Another example
	<pre>
int g() { return 1; }
int f(int x) { if (x == 0) then return g(); else return f(x- 1); }

int
main() {
return f(2);
}
	</pre>
  Draw the activation tree: <code>main-&gt;f2</code>, <code>f2-&gt;f1</code>, <code>f1-&gt;f0</code>, <code>f0-&gt;g</code>.
	<li>The activation tree depends on run-time behaviour (may be different for different inputs).
	<li><b>Since activations are properly nested, a stack can track the currently active activations</b>
	<ul>
		<li>The stack will not keep track of the entire activation tree
		<li>It will only keep track of the currently active activations
		<li>Work the first example showing the tree and the active lifetimes (as a stack) over runtime
	</ul>
	<li>Let's look at the memory layout, where the first portion is occupied by the code. One of the important structures that goes in the "data area" is the stack of active activations. The stack typically grows in one direction; usually downwards (by convention).
</ul>

<h3>Activation Records</h3>
<p>The active activations can be maintained in a stack.  The information for each activation is stored in an <em>activation record</em>, also called a <em>frame</em>.
What information to keep in an activation?
<ul>
	<li>The information needed to manage one procedure activation is called an <em>activation record</em> (AR) or <em>frame</em>
	<li>If procedure <code>F</code> calls procedure <code>G</code>, then <code>G</code>'s activation record contains a mix of info about <code>F</code> and <code>G</code>
	<li><code>F</code> is "suspended" until <code>G</code> completes, at which point <code>F</code> resumes
	<li><code>G's</code> AR contains information needed to
	<ul>
		<li>Complete execution of <code>G</code>
		<li>Resume execution of <code>F</code>
	</ul>
	<li>
<!--Example
	<pre>
int g() { return 1; }
int f(int x) { if (x == 0) then return g(); else return f(x- 1); }

int
main() {
return f(3);
}
	</pre>-->
	AR: (1) result , (2) arguments , (3) control-link : pointer to the caller's activation , (4) return address
	<li>Executing this program by hand, show the AR for each function (except main). There can be two return addresses for <code>f</code>, and show them as "*" and "**" respectively. These represent addresses of the instructions in memory.
	<li>This stack is not as abstract as a general stack. This stack has an array-like layout, and hence compiler writers will often play tricks to exploit this fact that activation records are adjacent in memory.
	<li>This is only one of many possible AR designs. e.g., many compilers don't use a control link, they rely on the fact that the stack is laid out linearly as an array. Similarly, the return address may be in a register (and not in the stack). The compiler determines the AR. Different compilers may employ different ARs. Some ARs are more efficient than others. Some ARs are easier to generate code for.
</ul>
<ul>
	<li>The advantage of placing the result in the first word of the AR is that the caller can find the result at a fixed offset from its own activation (size of its AR + 1).
	<li>Can divide responsibility between caller/callee differently
	<li>Real compilers make careful tradeoffs on which part of the activation frame should be in registers and which part in memory
  <li>The compiler must determine, at compile-time, the layout of activation records and generate code that correctly accesses locations in the activation record. Thus AR layout and the code generator must be designed together!
</ul>
<h3>Globals and Heap</h3>
<ul>
	<li>A global variable's lifetime transcends all procedure lifetimes. It has a global lifetime
	<ul>
		<li>Can't store a global in an activation record
	</ul>
	<li>Globals are assigned a fixed address once
	<ul>
		<li>Variables with fixed address are "statically allocated"
	</ul>
	<li>Depending on the language, there may be other statically allocated values
	<li>Memory layout: Code, static data, stack (grows downwards)
</ul>
<p>Heap objects
<ul>
	<li>A value that outlives the procedure that creates it cannot be kept in the AR
	<li>e.g., new A</code> should survive deallocation of the current procedure's AR.
</ul>

<p>Usually
<ul>
	<li>The code area contains object code: fixed size and read only
	<li>Static area contains data (not code) with fixed addresses: fixed size, may be readable or writeable
	<li>Stack contains an AR for each currently active procedure. Each AR usually fixed size, contains locals
	<li>Heap contains all other data: In C, heap is managed through <em>malloc</em> and <em>free</em>
</ul>

<p>Both heap and stack grow. Must take care they don't grow into each other. Simple solution: put them at opposite ends of memory and let them grow towards each other. If the two regions start touching each other, then the program is out of memory (or it may request more memory, etc.). This design allows different programs to use these different areas as they see fit, e.g., some programs may have large stacks, other may have large static data regions.

<h3>Alignment</h3>
Low-level but important detail that every compiler writer needs to be aware of.
<ul>
	<li>Most modern machines are 32 or 64 bit
	<ul>
		<li>8 bits in a byte
		<li>4 or 8 bytes in a word
		<li>Machines are either byte or word addressable
	</ul>
  <li>Did you know: C requires an object of type <code>int</code> to be aligned by <code>sizeof(int)</code>, which is typically <code>four</code>.  The compiler may need to generate padding bytes on stack to meet this requirement for local variables.
  <li>On the other hand, a hardware machine may have its own alignment requirements, e.g., for certain opcodes to be executed legally.
	<li>Data is <em>word aligned</em> if it begins at a word boundary, e.g., word=4bytes for 32-bit machine.
	<li>Most machines have some alignment restrictions
	<ul>
		<li>e.g., exception if access misaligned data
		<li>Or dramatic performance penalties for poor alignment (can be up to 10x depending on the machine)
	</ul>
	<li>Example: a string "Hello" takes 5 characters (without a terminating '\0').
	<ul>
		<li>Show an array of bytes with word-boundaries marked with darker lines
		<li>To word align next word, add 3 "padding" characters to the string
		<li>The padding is not part of the string, it's just unused memory
	</ul>
</ul>

<h3>Stack machines</h3>
Begin talking about code generation. Stack machines are the simplest model of code generation

<p>In a stack machine
<ul>
	<li>Only storage is a stack
	<li>An instruction
	<pre>
r = F(a1, ..., an):
	</pre>
	<ul>
		<li>Pop the top <code>n</code> words off the stack
		<li>Apply <code>f</code>
		<li>Push the result on the stack
	</ul>
	<li>Example: two instructions <code>push i</code> and <code>add</code>
	<li>Location of the operands/result is not explictly stated
	<ul>
		<li>Always the top of the stack
	</ul>
	<li>In contrast to a <em>register machine</em>
	<ul>
		<li><code>add</code> instead of <code>add r1,r2,r3</code>
		<li>More compact programs
		<li>One reason that Java bytecode uses stack evaluation model
		<ul>
			<li>In early days, Java was meant for mobility and memory was scarce
		</ul>
	</ul>
	<li>There is an intermediate point between a pure stack machine and a pure register machine
	<li>An <em>n-register stack machine</em>: Conceptually, keep the top <em>n</em>  locations of the stack in the registers
	<li>A one-register stack machine: the one register is called an <em>accumulator</em>.
	<li>Advantage of a one-register stack machine:
	<ul>
		<li>In a pure stack machine
		<ul>
			<li>An <code>add</code> does 3 memory operations (load two arguments and store one result
		</ul>
		<li>In a one-register stack machine
		<ul>
			<li>The add does <code>acc &lt;-- acc + top_of_stack</code>
			<li>Just one memory read
		</ul>
	</ul>
	<li>Consider an expression <code>op(e1,...,en)</code>
	<ul>
		<li>Note <code>e1,..,en</code> are subexpressions
	</ul>
	<li>For each <code>ei</code>, compute <code>ei</code> with the result in the accumulator; push the result on the stack
	<li>For <code>en</code>, just evaluate into accumulator, do not push on stack.
	<li>Evaluate <code>op</code> using values on stack and <code>en</code> in accumulator to produce result in accumulator
  <li>e.g., <code>7+5</code>: push 7; acc &lt;-- 5; acc &lt;-- acc + [top]
	<li>Invariant: after evaluating an expression <code>e</code>, the accumulator holds the value of <code>e</code> and the stack is unchanged
	<ul>
		<li>Important property: expression evaluation preserves the stack
	</ul>
	<li>Another example: 3 + (7 + 5)
	<ul>
		<li>First evaluate 3
		<li>Save 3
		<li>Evaluate 7
		<li>Save 7
		<li>Evaluate 5
		<li>Add
		<li>Add
	</ul>
	<li>Another important property: the expressions are being evaluated left-to-right. i.e., the evaluation order is left to right which also determines the order of the operands on the stack.
	<ul>
		<li> Some code generation strategies may depend on the evaluation order like this one
		<li> Others may not, e.g., where we had named identifiers (e.g., registers) storing the result for each intermediate expression
	</ul>
</ul>
