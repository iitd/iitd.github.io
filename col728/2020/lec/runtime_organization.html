<h3>Runtime Organization</h3>

<ul>
	<li>We have covered the front-end phases: lexing, parsing, semantic analysis
	<li>Now back-end: optimization, code generation
	<li>Before that: we need to talk about runtime organization --- what do we need to generate. Later we will talk about how to generate it (code generation)
</ul>

<p>Management of runtime resources
<ul>
	<li>Correspondence between static (compile-time) and dynamic (run-time) structures
	<ul>
		<li>What is done by the compiler, and what is deferred to the runtime
  </ul>
	<li>Storage organization
</ul>

<p>Execution of a program is initially under the control of the operating system
<ul>
	<li>The OS allocates space for a program
	<li>Code is loaded into part of the space
	<li>OS jumps to the entry point (e.g., "main")
</ul>

<p>Draw a picture of memory with "code" and "other space". Code is usually loaded at one end of the memory space (e.g., low or high end).

<ul>
	<li>Will draw memory as a rectangle with low address at bottom and high address at top
	<li>Lines delimiting different areas of the memory
	<li>Simplification: assume contiguous memory (need not be necessary)
</ul>

<ul>
	<li>Other space = Data space
	<li>Compiler is responsible for
	<ul>
		<li>Generating code
		<li>Orchestrating use of data area
	</ul>
</ul>

<h3>Activations</h3>
<ul>
	<li>Two goals
	<ul>
		<li>Correctness
		<li>Speed
	</ul>
	<li>Complications in code generation from trying to be fast as well as correct
	<li>Over time, fairly elaborate structures have been developed on how to do code generation and runtime structures
</ul>

<p>Two assumptions:
<ul>
	<li>Execution is sequential; control moves from one point in a program to another in a well-defined order
	<ul>
		<li>Violated in face of concurrency
	</ul>
	<li>When a procedure is called, control always returns to the point immediately after the call
	<ul>
		<li>Violated in catch/throw style exceptions (an exception may escape multiple procedures before it is caught)
		<li>Call/cc: call with current continuation
	</ul>
	<li>Even such violating constructs depend on the ideas discussed here
</ul>

<ul>
	<li>An invocation of procedure <code>P</code> is an <em>activation</em> of <code>P</code>
	<li>The <em>lifetime</em> of an activation of <code>P</code> is
	<ul>
		<li>All the steps to execution <code>P</code>
		<li>Including all the steps in procedures <code>P</code> calls
	</ul>
	<li>The <em>lifetime</em> of a variable <code>x</code> is the portion of execution in which <code>x</code> is defined
	<li>Note that
	<ul>
		<li>Lifetime is a dynamic (run-time) concept
		<li>Scope is a static concept
	</ul>
	<li>Observation
	<ul>
		<li>When P calls Q, then Q returns before P returns
	</ul>
	<li>Lifetimes of procedure activations are properly nested
	<li>Activation lifetimes can be depicted as a tree
	<li>Consider an example:
	<pre>
int g() { return 1; }
int f() { return g(); }

int
main() {
g();
f();
}
	</pre>
	Draw the tree of activation lifetimes
	<ul>
		<li>The example shows nesting and the fact that different calls to the same procedure will have different activations
	</ul>
	<li>Another example
	<pre>
int g() { return 1; }
int f(int x) { if (x == 0) then return g(); else return f(x- 1); }

int
main() {
return f(3);
}
	</pre>
  Draw the activation tree
	<li>The activation tree depends on run-time behaviour (may be different for different inputs).
	<li><b>Since activations are properly nested, a stack can track the currently active activations</b>
	<ul>
		<li>The stack will not keep track of the entire activation tree
		<li>It will only keep track of the currently active activations
		<li>Work the first example showing the tree and the stack incrementally over runtime
	</ul>
	<li>Let's look at the memory layout, where the first portion is occupied by the code. One of the important structures that goes in the "data area" is the stack of actie activations. The stack typically grows in one direction
</ul>

<h3>Activation Records</h3>
<p>What information to keep in an activation? That is what we will call the activation record.
<ul>
	<li>The information needed to manage one procedure activation is called an <em>activation record</em> (AR) or <em>frame</em>
	<li>If procedure <code>F</code> calls procedure <code>G</code>, then <code>G</code>'s activation record contains a mix of info about <code>F</code> and <code>G</code>
	<li><code>F</code> is "suspended" until <code>G</code> completes, at which point <code>F</code> resumes
	<li><code>G's</code> AR contains information needed to
	<ul>
		<li>Complete execution of <code>G</code>
		<li>Resume execution of <code>F</code>
	</ul>
	<li>Example
	<pre>
int g() { return 1; }
int f(int x) { if (x == 0) then return g(); else return f(x- 1); }

int
main() {
return f(3);
}
	</pre>
	AR: (1) result , (2) arguments , (3) control-link : pointer to the caller's activation , (4) return address
	<li>Executing this program by hand, show the AR for each function (except main). There can be two return addresses for <code>f</code>, and show them as "*" and "**" respectively. These represent addresses of the instructions in memory.
	<li>This stack is not as abstract as a general stack. This stack has an array-like layout, and hence compiler writers will often play tricks to exploit this fact that activation records are adjacent in memory.
	<li>This is only one of many possible AR designs. e.g., many compilers don't use a control link, they rely on the fact that the stack is laid out linearly as an array. Similarly, the return address may be in a register (and not in the stack). The compiler determines the AR. Different compilers may employ different ARs. Some ARs are more efficient than others. Some ARs are easier to generate code for.
</ul>
<ul>
	<li>The advantage of placing the result in the first word of the AR is that the caller can find the result at a fixed offset from its own activation (size of its AR + 1).
	<li>Can divide responsibility between caller/callee differently
	<li>Real compilers make careful tradeoffs on which part of the activation frame should be in registers and which part in memory
  <li>The compiler must determine, at compile-time, the layout of activation records and generate code that correctly accesses locations in the activation record. Thus AR layout and the code generator must be designed together!
</ul>
<h3>Globals and Heap</h3>
<ul>
	<li>A global variable's lifetime transcends all procedure lifetimes. It has a global lifetime
	<ul>
		<li>Can't store a global in an activation record
	</ul>
	<li>Global s are assigned a fixed address once
	<ul>
		<li>Variables with fixed address are "statically allocated"
	</ul>
	<li>Depending on the language, there may be other statically allocated values
	<li>Memory layout: Code, static data, stack (grows downwards)
</ul>
<p>Heap objects
<ul>
	<li>A value that outlives the procedure that creates it cannot be kept in the AR
	<li>e.g., new A</code> should survive deallocation of the current procedure's AR.
</ul>

<p>Usually
<ul>
	<li>The code area contains object code: fixed size and read only
	<li>Static area contains data (not code) with fixed addresses: fixed size, may be readable or writeable
	<li>Stack contains an AR for each currently active procedure. Each AR usually fixed size, contains locals
	<li>Heap contains all other data: In C, heap is managed through <em>malloc</em> and <em>free</em>
</ul>

<p>Both heap and stack grow. Must take care they don't grow into each other. Simple solution: put them at opposite ends of memory and let them grow towards each other. If the two regions start touching each other, then the program is out of memory (or it may request more memory, etc.). This design allows different programs to use these different areas as they see fit, e.g., some programs may have large stacks, other may have large static data regions.

<h3>Alignment</h3>
Low-level but important detail that every compiler writer needs to be aware of.
<ul>
	<li>Most modern machines are 32 or 64 bit
	<ul>
		<li>8 bits in a byte
		<li>4 or 8 bytes in a word
		<li>Machines are either byte or word addressable
	</ul>
	<li>Data is <em>word aligned</em> if it begins at a word boundary
	<li>Most machines have some alignment restrictions
	<ul>
		<li>E.g., undefined behaviour if access misaligned data
		<li>Or dramatic performance penalties for poor alignment (can be up to 10x depending on the machine)
	</ul>
	<li>Example: a string "Hello" takes 5 characters (without a terminating '\0').
	<ul>
		<li>Show an array of bytes with word-boundaries marked with darker lines
		<li>To word align next word, add 3 "padding" characters to the string
		<li>The padding is not part of the string, it's just unused memory
	</ul>
</ul>

<h3>Stack machines</h3>
Begin talking about code generation. Stack machines are the simplest model of code generation

<p>In a stack machine
<ul>
	<li>Only storage is a stack
	<li>An instruction
	<pre>
r = F(a1, ..., an):
	</pre>
	<ul>
		<li>Pop the top <code>n</code> words off the stack
		<li>Apply <code>f</code>
		<li>Push the result on the stack
	</ul>
	<li>Example: two instructions <code>push i</code> and <code>add</code>
	<li>Location of the operands/result is not explictly stated
	<ul>
		<li>Always the top of the stack
	</ul>
	<li>In contrast to a <em>register machine</em>
	<ul>
		<li><code>add</code> instead of <code>add r1,r2,r3</code>
		<li>More compact programs
		<li>One reason that Java bytecode uses stack evaluation model
		<ul>
			<li>In early days, Java was meant for mobility and memory was scarce
		</ul>
	</ul>
	<li>There is an intermediate point between a pure stack machine and a pure register machine
	<li>An <em>n-register stack machine</em>: Conceptually, keep the top <em>n</em>  locations of the stack in the registers
	<li>A one-register stack machine: the one register is called an <em>accumulator</em>.
	<li>Advantage of a one-register stack machine:
	<ul>
		<li>In a pure stack machine
		<ul>
			<li>An <code>add</code> does 3 memory operations (load two arguments and store one result
		</ul>
		<li>In a one-register stack machine
		<ul>
			<li>The add does <code>acc &lt;-- acc + top_of_stack</code>
			<li>Just one memory read
		</ul>
	</ul>
	<li>Consider an expression <code>op(e1,...,en)</code>
	<ul>
		<li>Note <code>e1,..,en</code> are subexpressions
	</ul>
	<li>For each <code>ei</code>, compute <code>ei</code> with the result in the accumulator; push the result on the stack
	<li>For <code>en</code>, just evaluate into accumulator, do not push on stack.
	<li>Evaluate <code>op</code> using values on stack and <code>en</code> in accumulator to produce result in accumulator
  <li>e.g., <code>7+5</code>: push 7; acc &lt;-- 5; acc &lt;-- acc + [top]
	<li>Invariant: after evaluating an expression <code>e</code>, the accumulator holds the value of <code>e</code> and the stack is unchanged
	<ul>
		<li>Important property: expression evaluation preserves the stack
	</ul>
	<li>Another example: 3 + (7 + 5)
	<ul>
		<li>First evaluate 3
		<li>Save 3
		<li>Evaluate 7
		<li>Save 7
		<li>Evaluate 5
		<li>Add
		<li>Add
	</ul>
	<li>Another important property: the expressions are being evaluated left-to-right. i.e., the evaluation order is left to right which also determines the order of the operands on the stack.
	<ul>
		<li> Some code generation strategies may depend on the evaluation order like this one
		<li> Others may not, e.g., where we had named identifiers (e.g., registers) storing the result for each intermediate expression
	</ul>
</ul>
