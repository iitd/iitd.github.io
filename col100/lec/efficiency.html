<h3>Understanding Program Efficiency</h3>
<ul>
<li> A modern computer can execute over a billion instructions per second.  So maybe program efficiency does not matter?
<ul>
<li> Some algorithms grow exponentially, e.g., finding prime numbers, compiler optimization algorithms</li>
<li> Need to tackle trillions of bytes of data, e.g., search engine</li>
<li> Some algorithms require both data and computation, e.g., large-language models</li>
</ul>
</li>
<li>There are multiple possible programs that achieve the same objective.  How can we decide which program is most efficient?</li>
</ul>


<p><b>Time vs. Space Efficiency</b>
<ul>
<li> Sometimes more space efficient algorithms are also more time efficient, e.g., fib-iterative is more space- and time-efficient than fib-recursive.</li>
<li> Sometimes, there is a tradeoff between space and time efficiency
<ul>
<li> Can create copies of inputs, or use large data structures for efficient lookup for an overall faster algorithm</li>
</ul>
</li>
<li> Can store 1000s of Gigabytes (trillions of bytes) in a modern computer's memory and disk </li>
<li> Will focus on time efficiency </li>
</ul>

<p>Want to understand efficiency of programs. But there are
challenges in understanding efficiency of solution to a
computational problem:
<ul>
<li>A program can be implemented in many different
ways</li>
<li> You can solve a problem using only a handful of
different algorithms</li>
<li>Would like to separate choices of implementation
from choices of more abstract algorithm</li>
</ul>

<p>How to evaluate efficiency of programs
<ul>
<li> Measure with a <em>timer</em></li>
<li> <em>Count</em> the operations</li>
<li> Abstract notion of <em>order of growth</em>
<ul>
<li> Will argue that this is the most appropriate ways of assessing
the impact of choices of algorithm in solving a problem; and measuring
the inherent difficulty in solving a problem
</li>
</ul>
</li>
</ul>

<p>Timing a program
<ul>
<li> use time module: <code>import time</code>
<ul>
<li> recall that importing means to bring in that class into your own file
</ul>
</li>
<li> Use the <code>clock</code> function to record the current physical time
<pre>
import time
def c2f(c):
  return c*9/5 + 32

t0 = time.clock()
c2f(100000)
t1 = time.clock() - t0
print("call took", t1, "s")
</pre>
</li>
</ul>

<p>Timing programs is inconsistent
<ul>
<li> Goal: to evaluate different algorithms</li>
<li> running time <em>varies between algorithms</em>.  <code>Good</code></li>
<li> running time <em>varies between implementations</em>.  <code>Bad</code></li>
<li> running time <em>varies between computers</em>.  <code>Bad</code></li>
<li> running time <em>is not predictable based on small inputs</em>.  <code>Bad</code></li>
<ul>
<li> time varies for different inputs but cannot really express a relationship between inputs and time.  <code>Bad</code></li>
</ul>
</li>
</ul>

<p>Counting operations
<ul>
<li> assume these steps take <em>constant time</em>:
<ul>
<li> mathematical operations</li>
<li> comparisons</li>
<li> assignments</li>
<li> accessing objects in memory</li>
<li> function call and return</li>
</ul>
</li>
<li> then count the number of operations executed as function of size of input</li>
</ul>
<pre>
import time
def c2f(c):
  return c*9/5 + 32  # 4 ops

def mysum(x):
  total = 0  # 1 op
  for i in range(x+1):  # loop x+1 times.  1 op for x+1
    total += i  # 2 ops
  return total  # 1 op

#mysum takes 2 + 3(x+1) ops
</pre>

<p>Counting operations is also inconsistent
<ul>
<li> Goal: to evaluate different algorithms</li>
<li> count <em>depends on algorithm</em>.  <code>Good</code></li>
<li> count <em>depends on implementation</em>.  <code>Bad in some contexts, good in others</code></li>
<li> count <em>independent of computers</em>.  <code>Good</code></li>
<li> no clear definition of <em>which operations</em> to count.  <code>Bad</code></li>
<li> count varies for different inputs and can come up with a relationship between inputs and time.  <code>Good!</code></li>
</ul>

<p>Still need a better way
<ul>
<li> timing and counting <em>evaluate implementations</em>.</li>
<li> timing <em>evaluates machines</em>.</li>
</ul>
<ul>
<li> want to <em>evaluate algorithm</em></li>
<li> want to <em>evaluate scalability</em></li>
<li> want to <em>evaluate in terms of input size</em></li>
</ul>


<p>Ideas
<ul>
<li>  Going to focus on idea of counting operations in an
algorithm, but not worry about small variations in
implementation (e.g., whether we take 3 or 4 primitive
operations to execute the steps of a loop) </li>
<li> Going to focus on how algorithm performs when size of
problem gets arbitarily large</li>
<li> Want to relate time needed to complete a computation,
measured this way, against the size of the input to the problem</li>
<li> Need to decide what to measure, given that actual number of steps
may depend on specifics of trial</li>
</ul>


<p>Need to choose which input to use to evaluate a function
<ul>
<li> want to express <em>efficiency in terms of size of input</em>, so need
to decide what your input is</li>
<li> could be an <em>integer</em>, e.g., <code>mysum(x)</code>.</li>
<li> could be <em>length of a list</em>, e.g., <code>listSum(ls)</code>.</li>
<li> <em>you decide</em> when multiple parameters to a function, e.g., <code>searchForElement(ls, e)</code>.
</ul>

<p>Different inputs change how the program runs
<ul>
<li> a function that searches for an element in a list
<pre>
def searchForElement(ls, e):
  for i in ls:
    if i == e:
      return True
  return False
</pre>
</li>
<li> when <code>e</code> is <em>first element</em> in the list: BEST CASE</li>
<li> when <code>e</code> is <em>not in the list</em>: WORST CASE</li>
<li> when <em>look through about half</em> of the elements in the list: AVERAGE CASE</li>
<li> want to measure this behaviour in a general way</li>
</ul>

<p>BEST, AVERAGE, WORST cases
<ul>
<li> suppose you are given a list <code>L</code> of some length <code>len(L)</code>.</li>
<li> <em>best case</em>: minimum running time over all possible inputs of
a given size, <code>len(L)</code>:
<ul>
<li> constant for <code>searchForElement</code></li>
<li> first element in any list</li>
</ul>
</li>
<li> <em>average case</em>: average running time over all possible inputs of
a given size, <code>len(L)</code>
<ul>
<li> May use a practical measure to define frequency of each possible input</li>
</ul>
</li>
<li> <em>worst case</em>: maximum running time over all possible inputs of
a given size, <code>len(L)</code>
<ul>
<li> linear in length of list for <code>searchForElement</code></li>
<li>must search entire list and not find it</li>
<li> <em>We will generally focus on this case</em></li>
</ul>
</li>
</ul>

<p><b>Orders of growth</b>. Goals:
<ul>
<li> Want to evaluate program's efficiency when <em>input is very big</em></li>
<li> Want to express the <em>growth of program's run time</em> as input size grows</li>
<li> Want to put an <em>upper bound</em> on growth --- as tight as possible</li>
<li> Do not need to be precise: "order of", not "exact" growth</li>
<li> We will look at <em>largest factors</em> in run time (which section of the program will take the longest to run?)</li>
<li> <em>Thus, generally, we want tight upper bound on growth, as function of size of input, in worst case</em> </li>
</ul>

<p>Measuring Order of Growth: Big Oh Notation
<ul>
<li> Big Oh notation measures an <em>upper bound on the asymptotic growth</em>, often called order of growth</li>
<li> Big Oh or O() is used to describe worst case
<ul>
<li> worst case occurs often and is the bottleneck when a program runs</li>
<li> express rate of growth of program relative to the input size</li>
<li> evaluate algorithm NOT machine or implementation</li>
</ul>
</ul>

<p>Exact steps vs. O():
<pre>
def fact_iter(n):
  """ assumes n an int &gt;= 0 """
  answer = 1
  while n &gt; 1:  # comparison is 1 step
    answer *= n #2 steps
    n -= 1  #2 steps
  return answer
</pre>
<ul>
<li> computes factorial</li>
<li> number of steps: 1 + 5n + 1</li>
<li> worst case asymptotic complexity: <code>O(n)</code>
<ul>
<li> ignore additive constants</li>
<li> ignore multiplicative constants</li>
</ul>
</li>
</ul>

<p>What does <code>O(n)</code> measure?
<ul>
<li> Interested in describing how amount of time needed 
grows as size of (input to) problem grows</li>
<li> Thus, given an expression for the number of operations needed to
compute an algorithm, want to know asymptotic behaviour as size of
problem gets large</li>
<li> Hence, will focus on term that grows most rapidly in a sum of terms</li>
<li> And will ignore multiplicative constants, since want to know how
rapidly time required increases as we increase size of input</li>
</ul>

<p>Simplification examples
<ul>
<li> drop constants and multiplicative factors</li>
<li>focus on <em>dominant terms</em></li>
</ul>
<pre>
O(n^2): n^2 + 2n + 2
O(n^2): n^2 + 10000000n + 3^10000
O(n): log(n) + n + 4
O(n log(n)): 0.0001*n*log(n) + 300n
O(3^n): 2n^30 + 3^n
</pre>

<p>Examples of Types of Order of Growth<br>
<img src="order-of-growth.png" alt="Order of Growth" width=600>

<p>Analyzing programs and their complexity
<ul>
<li> <em>combine</em> complexity classes
<ul>
<li> analyze statements inside functions</li>
<li> apply some rules, focus on dominant term</li>
</ul>
</li>
<li> <em>Law of addition</em> for O():
<ul>
<li> used with <em>sequential</em> statements</li>
<li> O(f(n)) + O(g(n)) is O(f(n)+g(n))</li>
<li>For example:
<pre>
for i in range(n):  #O(n)
  print('a')
for i in range(n*n):  #O(n^2)
  print('a')
</pre>
is <code>O(n)+O(n^2)</code> which is <code>O(n+n^2)</code> which is O(n^2) because of dominant term.
</li>
</ul>
</li>
</ul>

<p>Analyzing programs and their complexity
<ul>
<li> <em>combine</em> complexity classes
<ul>
<li> analyze statements inside functions</li>
<li> apply some rules, focus on dominant term</li>
</ul>
</li>
<li> <em>Law of multiplication</em> for O():
<ul>
<li> used with <em>nested</em> statements or loops.</li>
<li> <code>O(f(n))*O(g(n))</code> is <code>O(f(n)*g(n))</code>.</li>
<li> for example:
<pre>
for i in range(n):  #n loops, each O(n), for a total of O(n)*O(n)
  for j in range(n):
    print(a)
</pre>
<code>O(n)*O(n)</code> is <code>O(n*n)</code> is <code>O(n^2)</code> because
the outer loop goes <code>n</code> times and the inner loop goes <code>n</code>
times for every outer loop iteration.
</li>
</ul>
</li>
</ul>

<p>Complexity Classes
<ul>
<li> <code>O(1)</code> denotes constant running time</li>
<li> <code>O(log n)</code> denotes logarithmic running time</li>
<li> <code>O(n)</code> denotes linear running time</li>
<li> <code>O(n log(n))</code> denotes log-linear running time</li>
<li> <code>O(n^c)</code> denotes polynomial running time (<code>c</code> is a constant)</li>
<li> <code>O(c^n)</code> denotes exponential running time (<code>c</code> is a constant raised to a power based on size of input)</li>
</ul>

<p>Complexity classes ordered low to high<br>
<img src="complexity-classes.png" alt="Complexity classes" width=600>

<p>Complexity growth
<table>
<tr><td>Class</td> <td>n=10</td> <td>n=100</td> <td>n=1000</td> <td>n=1000000</td></tr>
<tr> <td>O(1)</td> <td>1</td> <td>1</td> <td>1</td> <td>1</td></tr>
<tr> <td>O(log n)</td> <td>1</td> <td>2</td> <td>3</td> <td>6</td></tr>
<tr> <td>O(n)</td> <td>10</td> <td>100</td> <td>1000</td> <td>1000000</td></tr>
<tr> <td>O(n log(n))</td> <td>10</td> <td>200</td> <td>3000</td> <td>6000000</td></tr>
<tr> <td>O(n^2)</td> <td>100</td> <td>10000</td> <td>1000000</td> <td>1000000000000</td></tr>
<tr> <td>O(2^n)</td> <td>1024</td> <td>1.2*10^31</td> <td>1.07*10^311</td> <td>forget it!</td></tr>
</table>

<p>Linear complexity: Simple iterative loop algorithms are typically linear in
complexity.

<p>Linear search on unsorted list
<pre>
def linearSearch(L, e):
  found = False
  for i in range(len(L)):
    if e == L[i]:
      found = True  # can speed up a little by returning True here
                    # but speed up doesn't impact worst case
  return found
</pre>
<ul>
<li> must look through all elements to decide it's not there</li>
<li> <code>O(len(L))</code> for the loop * O(1) to test if <code>e == L[i]</code>.  <em>Subtle</em>: assumes we can retrieve element of list in constant time.</li>
<ul>
<li> <code>O(1+4n+1)</code> = <code>O(4n+2)</code> = <code>O(n)</code>.</li>
</ul>
</li>
<li> overall complexity is <code>O(n)</code> where <code>n</code> is <code>len(L)</code>.</li>
</ul>

<p>Constant-time list access
<img src="constant-time-list-access.png" alt="Constant-time list access" width=600>

<p>Linear search on <em>sorted</em> list
<pre>
def search(L, e):
  for i in range(len(L)):
    if L[i] == e:
      return True
    if L[i] &gt; e:
      return False
  return False
</pre>
<ul>
<li> must only look until reach a number greater than <code>e</code></li>
<li> <code>O(len(L))</code> for the loop <code>* O(1)</code> to test if <code>e == L[i]</code>.  <em>Worst case will need to look at whole list</em>.</li>
<li> overall complexity is <code>O(n)</code> where <code>n</code> is <code>len(L)</code></li>
<li> NOTE: order of growth is same, though run time may differ for the two search methods (sorted and unsorted)</li>
</ul>

<p>Linear algorithm: add characters of a string, assumed to be composed of decimal digits
<pre>
def addDigits(s):
  val = 0
  for c in s:
    val += int(c)
  return val
</pre>
This is <code>O(len(s))</code>.

<p>Linear complexity example: complexity often depends on the number of iterations
<pre>
def fact_iter(n):
  prod = 1
  for i in range(1, n+1):
    prod *= i
  return prod
</pre>
<ul>
<li> number of times around loop is <code>n</code></li>
<li> number of operations inside loop is a constant (in this case, three; set i, multiply, set prod)
<ul>
<li> <code>O(1+3n+1)</code> is <code>O(3n+2)</code> is <code>O(n)</code></li>
</ul>
</li>
<li> Overall just <code>O(n)</code></li>
</ul>

<p>Nested loops
<ul>
<li> simple loops are linear in complexity</li>
<li> what about loops that have loops within them?</li>
</ul>

<p>Quadratic complexity: determine if one list is subset of second, i.e.,
every element of first, appears in second (assume no duplicates)
<pre>
def isSubset(L1, L2):
  for e1 in L1:
    matched = False
    for e2 in L2:
      if e1 == e2:
        matched = True
        break
    if not matched:
      return False
  return True
</pre>
<ul>
<li> Outer loop executed <code>len(L1)</code> times</li>
<li> Each iteration will execute inner loop up to <code>len(L2)</code> times, with constant number of operations: <code>O(len(L1)*len(L2))</code>.</li>
<li> Worst case when L1 and L2 same length, all of elements of L1 in L2. The outer loop iterates <code>O(len(L1))</code> times, and the inner loop iterates <code>O(len(L2)/2)</code> times on average (averaged over outer loop iterations). Given that the lists are of equal length, let <code>n=len(L1)=len(L2)</code>, then the worst-case running time of this program is <code>O(n^2)</code>.</li>
</ul>

<p>Quadratic complexity example: Find intersection of two lists, return a list with each element appearing only once.
<pre>
def intersect(L1, L2):
  tmp = []
  for e1 in L1:
    for e2 in L2:
      if e1 == e2:
        tmp.append(e1) #collect all intersections in tmp
        break
  res = []
  for e in tmp:
    if not(e in res):
      res.append(e)  #eliminate duplicates
  return res
</pre>
<ul>
<li> First nested loop takes <code>len(L1)*len(L2)</code> steps</li>
<li> Second loop takes at most <code>len(tmp)</code> steps, which is at most <code>len(L1)</code>.</li>
<li> Determining <code>e in res</code> takes at most <code>len(res)</code> steps which can be at most <code>min(len(L1),len(L2))</code>.</li>
<li> If we assume lists are of roughly same length, then the running time is <code>O(len(L1)^2)</code>.</li>
</ul>
